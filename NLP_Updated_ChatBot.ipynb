{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GPQ9sVdJHvO5"
      },
      "source": [
        "# Names of the students:\n",
        "* Omer shik - 318948460\n",
        "* Rotem Gonen - 318321064"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rtqgFItXQP8X"
      },
      "source": [
        "list of intents:\n",
        "\n",
        "-1 - can't identify\n",
        "\n",
        "1 - order flight\n",
        "\n",
        "2 - order taxi\n",
        "\n",
        "3 - order hotel\n",
        "\n",
        "4 - number of guests"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fy0TLpZrSCWI"
      },
      "source": [
        "list of states: \n",
        "\n",
        "0 - starting dialog\n",
        "\n",
        "1 order flight \n",
        "\n",
        "2 - order taxi\n",
        "\n",
        "3 - order hotel \n",
        "\n",
        "4 - number of guests\n",
        "\n",
        "5 - ending dialog"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ojs3fy04tz09"
      },
      "source": [
        "\n",
        "# Preparing the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzgaR05cJKLG",
        "outputId": "74728d0e-47c6-4efa-d107-6aa602284bfb"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import requests\n",
        "import json\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words and lemmatize the words\n",
        "    words_filtered = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the filtered words back into a sentence\n",
        "    return \" \".join(words_filtered)\n",
        "\n",
        "# Apply the preprocessing function to the text column of the DataFrame\n",
        "data['text'] = data['text'].apply(preprocess_text)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Du8FoWDvuF62"
      },
      "source": [
        "#The Training of the model and the evaluation of the outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "Yi0GKfJouEnY",
        "outputId": "ca8cb6d7-afab-4fa7-99a8-52a07235b1d0"
      },
      "outputs": [],
      "source": [
        "# # create vector based of the features \n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['text'])\n",
        "y = data['intent'].values\n",
        "\n",
        "# # split to train and test \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=12)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# Train a logistic regression model\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "pred = clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, pred, ax=ax)\n",
        "\n",
        "_ = ax.set_title(\n",
        "    f\"Confusion Matrix for {clf.__class__.__name__}\\non filtered documents\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T7UP5NGmvEvV"
      },
      "source": [
        "# Creating the polarity object "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xpi06ySwon7",
        "outputId": "195ca164-9a6b-4152-ed16-6f7a50e062f6"
      },
      "outputs": [],
      "source": [
        "!pip install spacytextblob\n",
        "import spacy\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe('spacytextblob')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MorVBKDfvZgq"
      },
      "source": [
        "# Using Word2vec spell check "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kilDtjXX7X_e"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "# Load the pre-trained Google News Word2Vec model\n",
        "model = api.load('glove-wiki-gigaword-100')\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGxaq1Pd_Izf"
      },
      "outputs": [],
      "source": [
        "def spell_check(text):\n",
        "    corrected_text = []\n",
        "    for word in text.split():\n",
        "        if word in model:\n",
        "            corrected_text.append(word)\n",
        "        else:\n",
        "            corrected_word = correction(word)\n",
        "            corrected_text.append(corrected_word)\n",
        "    return ' '.join(corrected_text)\n",
        "\n",
        "# Probability of `word`.\n",
        "def P(word):\n",
        "    return - model.key_to_index.get(word, 0).index if word in model.vocab else 0\n",
        "\n",
        "# Generate possible spelling corrections for word.\n",
        "def candidates(word):\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "# The subset of `words` that appear in the dictionary of WORDS.\n",
        "def known(words):\n",
        "    return set(w for w in words if w in model.vocab)\n",
        "\n",
        "# All edits that are one edit away from `word`.\n",
        "def edits1(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "## All edits that are two edits away from `word`.\n",
        "def edits2(word):\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "    \n",
        "def correction(word):\n",
        "    return max(candidates(word), key=P)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4dCQOqqXv2Wx"
      },
      "source": [
        "#Using OpenTripMap API\n",
        "in order to offer attractions based on the user input "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXkkQPMbUUlW"
      },
      "outputs": [],
      "source": [
        "def GetRecomendedAttractions(destination):\n",
        "\n",
        "  # Specify the API endpoint URL and parameters\n",
        "  api_key = \"5ae2e3f221c38a28845f05b60a50fcc28c951ccd419cde3ff204da68\"\n",
        "\n",
        "  destination = destination.replace(\" \", \"%20\")\n",
        "  destination = destination.title()\n",
        "  url = f\"https://api.opentripmap.com/0.1/en/places/geoname?name={destination}&apikey={api_key}\"\n",
        "  # Send a GET request to the API endpoint with the specified parameters\n",
        "  response = requests.get(url)\n",
        "  # Parse the JSON response\n",
        "  data = json.loads(response.text)\n",
        "  url = f\"https://api.opentripmap.com/0.1/en/places/radius?radius=20000&lon={data['lon']}&lat={data['lat']}&kinds=interesting_places&format=json&limit=15&apikey={api_key}\"\n",
        "  data = requests.get(url).json()\n",
        "  arr = []\n",
        "  for site in data:\n",
        "    if site[\"name\"] !='' and len(arr) < 3:\n",
        "      arr.append(site[\"name\"])\n",
        "  destination = destination.replace(\"%20\", \" \")\n",
        "  print(f\"This are the major attraction's in {destination}:\")\n",
        "  for i in range(len(arr)):\n",
        "    print(f\"{i+1}. {arr[i]}.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fJMnwhvYvnY0"
      },
      "source": [
        "#Chat bot implementation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXDSf_v4aSkt"
      },
      "outputs": [],
      "source": [
        "def process_chat_input(user_input):\n",
        "  # the default for unrecognized intent and \n",
        "  intent= -1\n",
        "  # spell check the input\n",
        "  text = spell_check(user_input)\n",
        "  output = None\n",
        "\n",
        "  doc = nlp(text)\n",
        "  # check if there is at least one intent with probablty above 0.5\n",
        "  new_text_transformed = vectorizer.transform([text])\n",
        "  ans = clf.predict_proba(new_text_transformed)\n",
        "  if any(x > 0.6 for x in ans[0]):\n",
        "    intent = clf.predict(new_text_transformed)\n",
        "  # transform the ['intent'] to 'intent'\n",
        "    intent = intent[0]\n",
        "  \n",
        "  # get all the entities that were found in the input\n",
        "  entities = {}\n",
        "  for ent in doc.ents:\n",
        "    entities[ent.label_] = ent.text\n",
        "  global next_step\n",
        "  # if there is a GPE in the input \n",
        "  if ('GPE' in entities.keys()): \n",
        "      output,next_step = next_state(intent,entities['GPE'])\n",
        "\n",
        "  # if there isn't a GPE in the input \n",
        "  else:\n",
        "      output,next_step = next_state(intent,None)\n",
        "  return output,intent,next_step, doc._.blob.polarity,entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVtsve0a2tt5"
      },
      "outputs": [],
      "source": [
        "def next_state(intent,gpe):\n",
        "  global option\n",
        "  if intent == -1:\n",
        "    return \"I'm having trouble understanding your request; please be more explicit.\",next_step\n",
        "  elif gpe != None and intent!='number of guests' and option:\n",
        "    GetRecomendedAttractions(gpe)\n",
        "    \n",
        "  if intent == 'order flight':\n",
        "    if gpe:\n",
        "      return f\"Your flight to {gpe} is booked!\",1\n",
        "      \n",
        "    else:\n",
        "      return f\"No problem i can order a flight for you\",1\n",
        "\n",
        "  if intent == 'order taxi':\n",
        "    if gpe:\n",
        "      return f\"Your taxi to {gpe} is on the way to your location\",2\n",
        "  \n",
        "    else:\n",
        "      return f\"No problem i can order a taxi for you\",2\n",
        "\n",
        "  if intent == 'order hotel':\n",
        "    x= 'Please insert how many adults and child are will be in the room?'\n",
        "    if gpe:\n",
        "      return f\"Your hotel in {gpe} is booked for your rqeuest. {x}\",3\n",
        "      \n",
        "    else:\n",
        "      return f\"No problem i can order an hotel room for you. {x}\",3\n",
        "\n",
        "  if intent == 'number of guests':\n",
        "    if gpe:\n",
        "      return f\"Your room hotel in {gpe} is booked for you\",4\n",
        "      \n",
        "    else:\n",
        "      return f\"No problem i ordered a room hotel for you\",4\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXTPq0vlIoBq"
      },
      "outputs": [],
      "source": [
        "def online():\n",
        "  global option\n",
        "  option = 1\n",
        "  next_step = 0\n",
        "  intro = 'Hi! I\\'m a travel chatbot, and I can help you book a flight, a taxi, or a hotel.\\nPlease include the name of the relevant city destination in the request for further offering.\\ninsert \"exit\" to quit.\\n'\n",
        "  print('Chatbot: ',intro)\n",
        "  while True:\n",
        "    user_input = input('User:')\n",
        "    user_input = str.lower(user_input)\n",
        "    if str.lower(user_input)=='exit':\n",
        "      break\n",
        "    print(process_chat_input(user_input),'\\n------------------------')\n",
        "  print('Thank you for using our travel bot!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf5-aASaOH9Q"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def offline(file_path):\n",
        "    # Open the input text file in read mode\n",
        "    with open(file_path,\"r\") as input_file:\n",
        "        # Read all lines from the file and store them as a list of strings\n",
        "        user_input_lines = input_file.readlines()\n",
        "\n",
        "    with open(\"output.csv\", \"w\", newline='') as output_file:\n",
        "        # Define the headers for the CSV file\n",
        "        headers = [\"Chatbot Answer\", \"Intent\", \"State\",\"Polarity\", \"Entities\"]\n",
        "        # Create a CSV writer object with headers\n",
        "        writer = csv.writer(output_file)\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        # Loop through all the lines in the input file\n",
        "        for line in user_input_lines:\n",
        "            line = str.rstrip(line)\n",
        "            # Process each line and get the result as a list\n",
        "            result = process_chat_input(str.lower(line))\n",
        "            # Write the result to the output CSV file\n",
        "            writer.writerow(result)\n",
        "\n",
        "    # Print a message to confirm that the text has been written to the file\n",
        "    print(\"The text has been written to output.csv\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6_IgPaDHSDCa"
      },
      "source": [
        "# Running the bot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h-cXbHmQC60X"
      },
      "source": [
        "## Offline state\n",
        "Run the cell below and pass the path of the file as string to the function.\n",
        "The result will be the file output.csv.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mo4YFxZQ_ke"
      },
      "outputs": [],
      "source": [
        "option = 0\n",
        "next_step = 0\n",
        "#offline('pass the path to the file here')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mO4eAgKWDXfc"
      },
      "source": [
        "## Online state\n",
        "Run the cell below to begin the chat,\n",
        "in order to see the use of the api in the chat there is need to put in the request the name of a city,(not all the city's are known by the api)\n",
        "for example: \"i need to fly to moscow\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt6nQyAfU0Nm",
        "outputId": "a0b5534e-d647-4bd7-813c-274262cb41e0"
      },
      "outputs": [],
      "source": [
        "next_step = 0\n",
        "online()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
